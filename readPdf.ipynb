{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got stderr: mar 18, 2025 9:49:56 A. M. org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
      "WARNING: Using fallback font 'Arial-BoldMT' for 'Century Gothic,Bold'\n",
      "\n",
      "C:\\Users\\venic\\AppData\\Local\\Temp\\ipykernel_12904\\1258841628.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtrado.replace(to_replace=r\"Unnamed.*\", value=np.nan, regex=True, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# setx JAVA_HOME \"C:\\Program Files\\Java\\jdk-23\"\n",
    "# setx PATH \"%PATH%;%JAVA_HOME%\\bin;%JAVA_HOME%\\bin\\server\"\n",
    "\n",
    "import jpype\n",
    "import pdfplumber as pdfl\n",
    "from tabula import read_pdf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pdf_path = \"REPORTE DE GACETA SEMANA DEL 17 AL 21 DE FEBRERO.pdf\"\n",
    "#Obtener el numero de paginas del pdf\n",
    "with pdfl.open(pdf_path) as pdf:\n",
    "    num_paginas = len(pdf.pages)\n",
    "\n",
    "# Lista para acumular los DataFrames de cada página\n",
    "dfs = []\n",
    "\n",
    "# Iterar sobre todas las páginas del PDF\n",
    "for hoja in range(1, num_paginas + 1):\n",
    "    # Definir el área de extracción (diferente para la primera página)\n",
    "    area = [100, 10, 800, 1000] if hoja == 1 else [10, 10, 800, 1000]\n",
    "\n",
    "    # Extraer la tabla de la página actual\n",
    "    df_pagina = read_pdf(pdf_path, pages=str(hoja), force_subprocess=True, encoding=\"latin1\", area=area, stream=True)\n",
    "\n",
    "    # Si `read_pdf()` devuelve una lista de DataFrames (varias tablas en una página), las combinamos\n",
    "    if isinstance(df_pagina, list):\n",
    "        df_pagina = pd.concat(df_pagina, ignore_index=True)\n",
    "\n",
    "\n",
    "    # Si es la primera hoja, guardamos el encabezado original\n",
    "    if hoja == 1:\n",
    "        df_pagina = df_pagina.drop(df_pagina.columns[5],axis=1)\n",
    "        encabezado_original = df_pagina.columns.tolist()  # Guardamos el encabezado real\n",
    "        dfAcumulador = df_pagina  # Guardamos la tabla completa\n",
    "    else:\n",
    "        # Guardar el encabezado detectado en la página como una nueva fila\n",
    "        df_encabezado_pagina = pd.DataFrame([df_pagina.columns.tolist()], columns=df_pagina.columns)\n",
    "        # Concatenar encabezado de la página como fila + los datos de la tabla\n",
    "        df_pagina = pd.concat([df_encabezado_pagina, df_pagina], ignore_index=True)\n",
    "        # Asegurar que todas las páginas tengan el mismo número de columnas\n",
    "        df_pagina.columns = encabezado_original  # Reasignamos el encabezado original\n",
    "        # Agregar la tabla procesada al acumulador\n",
    "        dfAcumulador = pd.concat([dfAcumulador, df_pagina], ignore_index=True)\n",
    "\n",
    "# Filtrar filas que contienen \"TOTAL\" en cualquier columna\n",
    "df_filtrado = dfAcumulador[~dfAcumulador.apply(lambda col: col.map(lambda x: isinstance(x, str) and \"total\" in x.lower())).any(axis=1)]\n",
    "df_filtrado.replace(to_replace=r\"Unnamed.*\", value=np.nan, regex=True, inplace=True)\n",
    "# Obtener los nombres de las dos primeras columnas dinámicamente\n",
    "col_0, col_1 = df_filtrado.columns[:2]\n",
    "# Identificar las filas desalineadas sin necesidad de conocer los nombres de las columnas\n",
    "mask_desalineadas = (\n",
    "    df_filtrado[col_0].isna() | \n",
    "    df_filtrado[col_1].isna() \n",
    ")\n",
    "\n",
    "# Fusionar solo los valores de las filas desalineadas con la fila anterior en la misma columna\n",
    "for i in range(1, len(df_filtrado)):\n",
    "    if mask_desalineadas.iloc[i]:  # Si la fila está desalineada\n",
    "        for col in df_filtrado.columns:  # Iteramos por cada columna\n",
    "            if pd.notna(df_filtrado.iloc[i, df_filtrado.columns.get_loc(col)]):  # Si hay datos en la celda\n",
    "                df_filtrado.iloc[i - 1, df_filtrado.columns.get_loc(col)] += \" \" + df_filtrado.iloc[i, df_filtrado.columns.get_loc(col)]  # Fusionar con la anterior\n",
    "\n",
    "# Eliminar las filas desalineadas después de fusionarlas\n",
    "df_filtrado = df_filtrado[~mask_desalineadas].reset_index(drop=True)\n",
    "# Guardar el DataFrame corregido en CSV\n",
    "df_filtrado.to_csv(\"datos_extraidos.csv\", index=False, sep=\";\")\n",
    "# df_filtrado"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
